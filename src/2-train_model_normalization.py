import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# âœ… Step 1: Load the feature dataset
data_file = "output/221IT047_URLfeaturedataset.csv"
data = pd.read_csv(data_file)

# ðŸ” Ensure 'label' column exists
if "label" not in data.columns:
    raise ValueError("ðŸš¨ ERROR: 'label' column is missing in the dataset!")

# âœ… Step 2: Separate features (X) and labels (y)
X = data.drop(columns=["label"])  # Features
y = data["label"]  # Labels

# ðŸ” Ensure no missing values
if X.isnull().sum().sum() > 0 or y.isnull().sum() > 0:
    raise ValueError("ðŸš¨ ERROR: Missing values detected in dataset!")

# âœ… Step 3: Validate dataset shape
if X.shape[0] != y.shape[0]:
    raise ValueError(f"ðŸš¨ ERROR: X has {X.shape[0]} samples, but y has {y.shape[0]}!")

print(f"âœ… Data Loaded Successfully! X shape: {X.shape}, y shape: {y.shape}")

# âœ… Step 4: Normalize Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# âœ… Step 5: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

# âœ… Step 6: Build the Model
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')  # Binary classification
])

# âœ… Step 7: Compile Model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# âœ… Step 8: Train the Model
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=32,
    validation_data=(X_test, y_test),
    verbose=1
)

# âœ… Step 9: Save Model & Scaler
model.save("output/NIDHI-221IT047-model.h5")
np.save("output/scaler.npy", scaler.mean_)  # Save scaler parameters

print("ðŸŽ‰ Model training complete! âœ… Model and scaler saved successfully.")
